---
title: "Lab4_InClass_2"
author: "Mateo Robbins"
date: "2023-05-02"
output: html_document
---

```{r packages, include = FALSE}
library(tidytext)
library(tidyverse)
library(tidymodels)
library(textrecipes)
library(discrim) # naive-bayes
```

This data set includes more possible predictors than the text alone, but for this model we will only use the text variable
```{r data}

urlfile ="https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv"
incidents_df<-readr::read_csv(url(urlfile))
```

Now we'll split our data into training and test portions

```{r split-data}
set.seed(1234)
incidents2class <- incidents_df %>%
  mutate(fatal = factor(if_else(
    is.na(Deadly) ,
    "non-fatal", "fatal")))

incidents_split <- initial_split(incidents2class, strata = fatal)

incidents_train <- training(incidents_split)
incidents_test <- testing(incidents_split)

```

We use recipe() to specify the predictor and outcome variables and the data.

```{r recipe}
incidents_rec <- recipe(fatal ~ Text, data = incidents_train)
```

Next we add some familiar pre-processing steps on our Text variable: tokenize to word level, filter to the most common words, and calculate tf-idf.

```{r pre-process}
recipe <- incidents_rec %>%
  step_tokenize(Text) %>%
  step_tokenfilter(Text, max_tokens = 1000) %>%
  step_tfidf(Text) #new one from textrecipes
```

Create  tidymodels workflow to combine the modeling components

The advantages are: You don't have to keep track of separate objects in your workspace. The recipe prepping and model fitting can be executed using a single call to fit() . If you have custom tuning parameter settings, these can be defined using a simpler interface when combined with tune.
```{r workflow}
incidents_wf <- workflow() %>%
  add_recipe(recipe)
```

```{r nb-spec}
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>% #set modeling context
  set_engine("naivebayes") #method for fitting model

nb_spec
```

Now we are ready to add our model to the workflow and fit it to the training data

```{r fit-model}
nb_fit <- incidents_wf %>%
  add_model(nb_spec) %>%
  fit(data = incidents_train)
```
Next up is model evaluation. We'll stretch our training data further and use resampling to evaluate our naive Bayes model. Here we create 10-fold cross-validation sets, and use them to estimate performance.

```{r}
set.seed(234)
incidents_folds <- vfold_cv(incidents_train) #default is v = 10

incidents_folds
```

```{r nb-workflow}
nb_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(nb_spec)

nb_wf
```

To estimate its performance, we fit the model many times, once to each of these resampled folds, and then evaluate on the heldout part of each resampled fold.

```{r fit-resamples}
nb_rs <- fit_resamples(
  nb_wf,
  incidents_folds,
  control = control_resamples(save_pred = TRUE)
)
```

Extract the relevant information using collect_metrics() and collect_predictions() and examine the performance metrics.

```{r performance}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)
nb_rs_metrics
```

We'll use two performance metrics: accuracy and ROC AUC.
Accuracy is the proportion of the data that is predicted correctly. 
The ROC curve plots the true positive rate against the false positive rate; AUC closer to 1 indicates a better-performing model, while AUC closer to 0.5 indicates a model that does no better than random guessing.

```{r performance-plot}
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = fatal, .pred_fatal) %>%
  autoplot() +
  labs(
    "Resamples",
    title = "ROC curve for Climbing Incident Reports"
  )
```

Another model method involves the confusion matrix. A confusion matrix tabulates a model's false positives and false negatives for each class.

```{r confusion-matrix}
conf_mat_resampled(nb_rs, tidy = FALSE) %>% #compute matrix for each fold then average
  autoplot(type = "heatmap")
```

Let's move up to a more sophisticated model. Recall that lasso classification model uses regularization on regression to help us choose a simpler, more generalizable model.  Variable selection helps us identify which features to include in our model.

Lasso classification learns how much of a penalty to put on features to reduce the high-dimensional space of original possible variables (tokens) for the final model.

```{r lasso-specification}
lasso_spec <- logistic_reg(penalty = 0.01, mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

lasso_spec
```

```{r lasso-workflow}
lasso_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(lasso_spec)

lasso_wf
```

```{r fit-resamples-lasso}
set.seed(123)
lasso_rs <- fit_resamples(
  lasso_wf,
  incidents_folds,
  control = control_resamples(save_pred = T)
)
#pull out metric and prediction
lasso_rs_metrics <- collect_metrics(lasso_rs)
lasso_rs_predictions <- collect_predictions(lasso_rs)

lasso_rs_metrics

```

```{r lasso-plot}

lasso_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = fatal, .pred_fatal) %>%
  autoplot() +
  labs(
    color = "Resamples",
    title = "ROC for Climbing Incident Reports"
  )
```

```{r lasso-conf-mat}
conf_mat_resampled(lasso_rs, tidy = F) %>%
  autoplot(type = "heatmap")
```

The value penalty = 0.01 is a model hyperparameter. The higher it is, the more model coefficients are reduced (sometimes to 0, removing them -- feature selection). We set it manually before, but we can also estimate its best value, again by training many models on resampled data sets and examining their performance.

```{r penalty-tuning-specification}
tune_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

tune_spec

```

```{r lamba}
lambda_grid <- grid_regular(penalty(), levels = 30)

lambda_grid

```

Here we use grid_regular() to create 30 possible values for the regularization penalty. Then tune_grid() fits a model at each of those values.


```{r tune}
tune_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(tune_spec)

set.seed(2023)
tunes_rs <- tune_grid(
  tune_wf,
  incidents_folds,
  grid = lambda_grid,
  control = control_resamples(save_pred = T)
  
)

```

```{r plot_metrics}
collect_metrics(tune_rs)
autoplot(tune_rs) + 
  labs(
    title = "Lasso performance across regularization penalties"
  )
```

```{r penalty-show-best}

tunes_rs %>%
  show_best("roc_auc")

tunes_rs %>%
  show_best("accuracy")

chosen_acc <- tune_rs %>%
  select_by_one_std_err(metric = "accuracy", -penalty)
```

Next, let's finalize our workflow with this particular regularization penalty. This is the regularization penalty that our tuning results indicate give us the best model.

```{r final-model}
final_lasso <- finalize_workflow(tune_wf, chosen_acc)

final_lasso

```

The penalty argument value now reflects our tuning result. Now we fit to our training data.

```{r}
fitted_lasso <- fit(final_lasso, incidents_train)

fitted_lasso
```

First let's look at the words associated with an accident being non-fatal.

```{r words-non-fatal}
fitted_lasso %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  arrange(-estimate)

```

And now the words that are most associated with a fatal incident.

```{r words-fatal}
fitted_lasso %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  arrange(estimate)
```

Finally, let's fit to the test data and see how we did.
```{r}
last_fit(final_lasso, incidents_split) %>%
  collect_metrics()
```

Lab 4 Assignment: Due May 9 at 11:59pm

1. Select another classification algorithm

2. Conduct an initial out-of-the-box model fit on the training data and prediction on the test test data.  Assess the performance of this initial model.

3. Select the relevant hyperparameters for your algorithm and tune your model.

4. Conduct a model fit using your newly tuned model specification.  What are the terms most highly associated with non-fatal reports?  What about fatal reports? 

5. Predict fatality of the reports in the test set.  Compare this prediction performance to that of the Naive Bayes and Lasso models.  


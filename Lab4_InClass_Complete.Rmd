---
title: "Lab4_InClass_complete"
author: "Mateo Robbins"
date: "2023-05-02"
output: html_document
---

```{r packages, include = FALSE}
library(tidytext)
library(tidyverse)
library(tidymodels)
library(textrecipes)
library(discrim) # naive-bayes
```

This data set includes more possible predictors than the text alone, but for this model we will only use the text variable
```{r data}

urlfile ="https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv"
incidents_df<-readr::read_csv(url(urlfile))
```

Now we'll split our data into training and test portions

```{r split-data}
set.seed(1234)
incidents2class <- incidents_df %>%
  mutate(fatal = factor(if_else(
    is.na(Deadly) ,
    "non-fatal", "fatal")))

incidents_split <- initial_split(incidents2class, strata = fatal)

incidents_train <- training(incidents_split)
incidents_test <- testing(incidents_split)

```

We use recipe() to specify the predictor and outcome variables and the data.

```{r recipe}
incidents_rec <- recipe(fatal ~ Text, data = incidents_train)
```

Next we add some familiar pre-processing steps on our Text variable: tokenize to word level, filter to the most common words, and calculate tf-idf.

```{r pre-process}
recipe <- incidents_rec %>%
  step_tokenize(Text) %>%
  step_tokenfilter(Text, max_tokens = 1000) %>%
  step_tfidf(Text) #new one from textrecipes
```

Create  tidymodels workflow to combine the modeling components

The advantages are: You don't have to keep track of separate objects in your workspace. The recipe prepping and model fitting can be executed using a single call to fit() . If you have custom tuning parameter settings, these can be defined using a simpler interface when combined with tune.
```{r workflow}
incidents_wf <- workflow() %>%
  add_recipe(recipe)
```

```{r nb-spec}
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>% #set modeling context
  set_engine("naivebayes") #method for fitting model

nb_spec
```

Now we are ready to add our model to the workflow and fit it to the training data

```{r fit-model}
nb_fit <- incidents_wf %>%
  add_model(nb_spec) %>%
  fit(data = incidents_train)
```
Next up is model evaluation. We'll stretch our training data further and use resampling to evaluate our naive Bayes model. Here we create 10-fold cross-validation sets, and use them to estimate performance.

```{r}
set.seed(234)
incidents_folds <- vfold_cv(incidents_train) #default is v = 10

incidents_folds
```

```{r nb-workflow}
nb_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(nb_spec)

nb_wf
```

To estimate its performance, we fit the model many times, once to each of these resampled folds, and then evaluate on the heldout part of each resampled fold.

```{r fit-resamples}
nb_rs <- fit_resamples(
  nb_wf,
  incidents_folds,
  control = control_resamples(save_pred = TRUE)
)
```

Extract the relevant information using collect_metrics() and collect_predictions() and examine the performance metrics.

```{r performance}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)
nb_rs_metrics
```

We'll use two performance metrics: accuracy and ROC AUC.
Accuracy is the proportion of the data that is predicted correctly. 
The ROC curve plots the true positive rate against the false positive rate; AUC closer to 1 indicates a better-performing model, while AUC closer to 0.5 indicates a model that does no better than random guessing.

```{r performance-plot}
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = fatal, .pred_fatal) %>%
  autoplot() +
  labs(
    "Resamples",
    title = "ROC curve for Climbing Incident Reports"
  )
```

Another model method involves the confusion matrix. A confusion matrix tabulates a model's false positives and false negatives for each class.

```{r confusion-matrix}
conf_mat_resampled(nb_rs, tidy = FALSE) %>% #compute matrix for each fold then average
  autoplot(type = "heatmap")
```
